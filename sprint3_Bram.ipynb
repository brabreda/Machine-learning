{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "We start of by importing the libraries that we need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 01:05:46.925721: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: images anomaly detection\n",
    "When we look at the pictures we see alot of dishes and interior/exterior photo's but there are also alot of unhelpfull pictures. These pictures include logo's, people and other random images. There isn't any benifit of having these pictures on a tripadvisor page. Therefor we call these anomalies, these pictures have no need to be on a tripadvisor page, as they bring no information to the customer. Because of this it would be nice to detect these images, this model could then be used to clear up the pages of tripadvisor. \n",
    "\n",
    "The detection of these images is called anomaly detection, there are 2 sorts of anomaly detection:\n",
    "- **Outlier detection:** For this model we need a dataset with both standard pictures and anomaly pictures. \n",
    "- **Novelty detection:** For this nodel the trainingset consists only of the standard pictures. The trainingset must be labeled, so it the models are in a supervised fashion.\n",
    "\n",
    "The dataset of tripadvisor is not labeled, therefor we could use outlier detection. Another choice can be to create a dataset of interiors, exteriors and dishes. This dataset would allow us to use **novelty detection**. For this case we will start of by looking at novelty detection.\n",
    "\n",
    "\n",
    "\n",
    "sources: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction the dataset\n",
    "\n",
    "The dataset will look like a array of 3 d matrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo adding labels\n",
    "def imagesInFolderToDataset(path, width, height, channels):\n",
    "    fileNameList = glob.glob(f\"{path}*\")\n",
    "    images = []\n",
    "    for fileName in tqdm(fileNameList, total=len(fileNameList)):\n",
    "        try:\n",
    "\n",
    "            img = Image.open(f\"{fileName}\")\n",
    "            img_np = np.array(img.resize(( width, height ), channels ))\n",
    "            if img_np.shape == (width,height,channels):\n",
    "                images.append(img_np)\n",
    "\n",
    "\n",
    "\n",
    "        except PIL.UnidentifiedImageError:\n",
    "            pass\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513/513 [00:04<00:00, 118.46it/s]\n",
      "100%|██████████| 111/111 [00:00<00:00, 148.65it/s]\n",
      "100%|██████████| 15183/15183 [01:02<00:00, 242.71it/s]\n"
     ]
    }
   ],
   "source": [
    "restaurants_train   = imagesInFolderToDataset(\"Images/restaurant/\", 64, 64, 3)\n",
    "buffet_train        = imagesInFolderToDataset(\"Images/buffet/\", 64, 64, 3)\n",
    "\n",
    "# create test and train set\n",
    "x_train     = np.concatenate((restaurants_train, buffet_train), axis=0)\n",
    "x_test      = imagesInFolderToDataset(\"tripadvisor_dataset/tripadvisor_images/\",64,64,3)\n",
    "\n",
    "# normalize the data\n",
    "x_train = x_train.astype(float) / 255.\n",
    "x_test  = x_test.astype(float) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te beginnen gaan we greyscale gebruiken, de reden hiervoor is dat we op deze manier de dimensionaliteit reduceren. Dit zorgt dat er minder kans is op overfitting. Fotos met rgb maken het wel mogelijk om een beter model te trainen maar hebben meer kans op overfitting, de oplossingen hiervoor zijn dim reduction of meer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_greyscale   = np.array(list(map( lambda x: np.dot(x[...,:3], [0.2989, 0.587, 0.114]), x_train)))\n",
    "x_test_greyscale    = np.array(list(map( lambda x: np.dot(x[...,:3], [0.2989, 0.587, 0.114]), x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convalutional autoencoder\n",
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=(64, 64, 1)),\n",
    "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = AnomalyDetector()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 2s 166ms/step - loss: 0.0667\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 1s 204ms/step - loss: 0.0660\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.0648\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.0631\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 0.0605\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.0567\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.0515\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 1s 190ms/step - loss: 0.0449\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 0.0370\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 1s 211ms/step - loss: 0.0283\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 0.0207\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.0167\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.0158\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0149\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 2s 348ms/step - loss: 0.0135\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 2s 399ms/step - loss: 0.0125\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.0119\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.0115\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.0111\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 0.0108\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(x_train_greyscale, x_train_greyscale, \n",
    "          epochs=20, \n",
    "          batch_size=128,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15169, 64, 64, 1), dtype=float32, numpy=\n",
       "array([[[[0.38968173],\n",
       "         [0.48573735],\n",
       "         [0.6419693 ],\n",
       "         ...,\n",
       "         [0.33125523],\n",
       "         [0.39581877],\n",
       "         [0.43386334]],\n",
       "\n",
       "        [[0.46167764],\n",
       "         [0.64270973],\n",
       "         [0.7464678 ],\n",
       "         ...,\n",
       "         [0.32229954],\n",
       "         [0.35104614],\n",
       "         [0.39644054]],\n",
       "\n",
       "        [[0.5767327 ],\n",
       "         [0.82396454],\n",
       "         [0.8896123 ],\n",
       "         ...,\n",
       "         [0.3935624 ],\n",
       "         [0.41239172],\n",
       "         [0.4380607 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6436168 ],\n",
       "         [0.8265079 ],\n",
       "         [0.86373717],\n",
       "         ...,\n",
       "         [0.917894  ],\n",
       "         [0.8735098 ],\n",
       "         [0.7604954 ]],\n",
       "\n",
       "        [[0.5752002 ],\n",
       "         [0.7860461 ],\n",
       "         [0.8839731 ],\n",
       "         ...,\n",
       "         [0.89145803],\n",
       "         [0.81598556],\n",
       "         [0.71655196]],\n",
       "\n",
       "        [[0.54113513],\n",
       "         [0.6321665 ],\n",
       "         [0.6691126 ],\n",
       "         ...,\n",
       "         [0.7688103 ],\n",
       "         [0.6731208 ],\n",
       "         [0.5859966 ]]],\n",
       "\n",
       "\n",
       "       [[[0.38689572],\n",
       "         [0.39271346],\n",
       "         [0.4499426 ],\n",
       "         ...,\n",
       "         [0.39944485],\n",
       "         [0.39054665],\n",
       "         [0.41464975]],\n",
       "\n",
       "        [[0.37795633],\n",
       "         [0.3762773 ],\n",
       "         [0.41958335],\n",
       "         ...,\n",
       "         [0.33785558],\n",
       "         [0.31899124],\n",
       "         [0.36162224]],\n",
       "\n",
       "        [[0.38757765],\n",
       "         [0.43676344],\n",
       "         [0.49767223],\n",
       "         ...,\n",
       "         [0.32438728],\n",
       "         [0.31958854],\n",
       "         [0.37669784]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.35714498],\n",
       "         [0.29632688],\n",
       "         [0.2936513 ],\n",
       "         ...,\n",
       "         [0.34091374],\n",
       "         [0.34481788],\n",
       "         [0.3633827 ]],\n",
       "\n",
       "        [[0.34609434],\n",
       "         [0.3058972 ],\n",
       "         [0.33699772],\n",
       "         ...,\n",
       "         [0.3408115 ],\n",
       "         [0.33487496],\n",
       "         [0.3851624 ]],\n",
       "\n",
       "        [[0.38687953],\n",
       "         [0.32428697],\n",
       "         [0.32725537],\n",
       "         ...,\n",
       "         [0.35520604],\n",
       "         [0.34947613],\n",
       "         [0.37990913]]],\n",
       "\n",
       "\n",
       "       [[[0.38576427],\n",
       "         [0.38399458],\n",
       "         [0.43207633],\n",
       "         ...,\n",
       "         [0.43196595],\n",
       "         [0.43247008],\n",
       "         [0.44690964]],\n",
       "\n",
       "        [[0.37134624],\n",
       "         [0.35330555],\n",
       "         [0.38432708],\n",
       "         ...,\n",
       "         [0.44797364],\n",
       "         [0.38969198],\n",
       "         [0.41981882]],\n",
       "\n",
       "        [[0.3796454 ],\n",
       "         [0.39585614],\n",
       "         [0.4381484 ],\n",
       "         ...,\n",
       "         [0.4924744 ],\n",
       "         [0.45406976],\n",
       "         [0.46663463]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.31257135],\n",
       "         [0.22824176],\n",
       "         [0.2313107 ],\n",
       "         ...,\n",
       "         [0.46777144],\n",
       "         [0.43339208],\n",
       "         [0.42987278]],\n",
       "\n",
       "        [[0.32291743],\n",
       "         [0.26502642],\n",
       "         [0.2937337 ],\n",
       "         ...,\n",
       "         [0.47513822],\n",
       "         [0.43539524],\n",
       "         [0.45229736]],\n",
       "\n",
       "        [[0.37002394],\n",
       "         [0.29821044],\n",
       "         [0.30751657],\n",
       "         ...,\n",
       "         [0.4436553 ],\n",
       "         [0.4150661 ],\n",
       "         [0.42287534]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.39975527],\n",
       "         [0.45021608],\n",
       "         [0.5538316 ],\n",
       "         ...,\n",
       "         [0.58811396],\n",
       "         [0.59692717],\n",
       "         [0.5608473 ]],\n",
       "\n",
       "        [[0.4376241 ],\n",
       "         [0.520712  ],\n",
       "         [0.60262775],\n",
       "         ...,\n",
       "         [0.7832389 ],\n",
       "         [0.6478863 ],\n",
       "         [0.63275   ]],\n",
       "\n",
       "        [[0.5124954 ],\n",
       "         [0.670527  ],\n",
       "         [0.74619234],\n",
       "         ...,\n",
       "         [0.88785106],\n",
       "         [0.8270009 ],\n",
       "         [0.72279924]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4369571 ],\n",
       "         [0.45592263],\n",
       "         [0.49953946],\n",
       "         ...,\n",
       "         [0.54158264],\n",
       "         [0.5183424 ],\n",
       "         [0.47230753]],\n",
       "\n",
       "        [[0.4069302 ],\n",
       "         [0.46584198],\n",
       "         [0.55906194],\n",
       "         ...,\n",
       "         [0.5178314 ],\n",
       "         [0.47702274],\n",
       "         [0.4795211 ]],\n",
       "\n",
       "        [[0.43387902],\n",
       "         [0.41562304],\n",
       "         [0.4418767 ],\n",
       "         ...,\n",
       "         [0.47294506],\n",
       "         [0.43884015],\n",
       "         [0.4361202 ]]],\n",
       "\n",
       "\n",
       "       [[[0.3979955 ],\n",
       "         [0.40695617],\n",
       "         [0.4607427 ],\n",
       "         ...,\n",
       "         [0.32193473],\n",
       "         [0.33308098],\n",
       "         [0.3774207 ]],\n",
       "\n",
       "        [[0.3945238 ],\n",
       "         [0.40134493],\n",
       "         [0.44171134],\n",
       "         ...,\n",
       "         [0.21398066],\n",
       "         [0.23656315],\n",
       "         [0.30248487]],\n",
       "\n",
       "        [[0.4172387 ],\n",
       "         [0.46695322],\n",
       "         [0.5145304 ],\n",
       "         ...,\n",
       "         [0.17937276],\n",
       "         [0.19898336],\n",
       "         [0.28760725]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.37466353],\n",
       "         [0.32763523],\n",
       "         [0.32184607],\n",
       "         ...,\n",
       "         [0.20551182],\n",
       "         [0.23074275],\n",
       "         [0.2834723 ]],\n",
       "\n",
       "        [[0.3532288 ],\n",
       "         [0.31689233],\n",
       "         [0.35360414],\n",
       "         ...,\n",
       "         [0.21343975],\n",
       "         [0.22942555],\n",
       "         [0.30447337]],\n",
       "\n",
       "        [[0.39092702],\n",
       "         [0.33179328],\n",
       "         [0.33350596],\n",
       "         ...,\n",
       "         [0.2656934 ],\n",
       "         [0.2808187 ],\n",
       "         [0.33248547]]],\n",
       "\n",
       "\n",
       "       [[[0.39330855],\n",
       "         [0.45413086],\n",
       "         [0.56814283],\n",
       "         ...,\n",
       "         [0.50427395],\n",
       "         [0.5137273 ],\n",
       "         [0.5035696 ]],\n",
       "\n",
       "        [[0.4374966 ],\n",
       "         [0.5484296 ],\n",
       "         [0.63343745],\n",
       "         ...,\n",
       "         [0.61204827],\n",
       "         [0.5188228 ],\n",
       "         [0.5228698 ]],\n",
       "\n",
       "        [[0.517269  ],\n",
       "         [0.70927453],\n",
       "         [0.78275126],\n",
       "         ...,\n",
       "         [0.7099307 ],\n",
       "         [0.6532173 ],\n",
       "         [0.5965713 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.51240164],\n",
       "         [0.58255446],\n",
       "         [0.6095042 ],\n",
       "         ...,\n",
       "         [0.73609906],\n",
       "         [0.69891196],\n",
       "         [0.6000021 ]],\n",
       "\n",
       "        [[0.45535144],\n",
       "         [0.5479444 ],\n",
       "         [0.6469799 ],\n",
       "         ...,\n",
       "         [0.69650286],\n",
       "         [0.62981564],\n",
       "         [0.58180165]],\n",
       "\n",
       "        [[0.46137506],\n",
       "         [0.46370563],\n",
       "         [0.48750043],\n",
       "         ...,\n",
       "         [0.59995544],\n",
       "         [0.54004264],\n",
       "         [0.5004725 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder(x_test_greyscale)\n",
    "\n",
    "reconstructions = model(data)\n",
    "loss = tf.keras.losses.mae(reconstructions, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze kunnen anomely detection opdelen in 2 verschillende delen:\n",
    "- **Outlier detection:** Our input dataset contains examples of both standard events and anomaly events. These algorithms seek to fit regions of the training data where the standard events are most concentrated, disregarding, and therefore isolating, the anomaly events. Such algorithms are often trained in an unsupervised fashion (i.e., without labels). We sometimes use these methods to help clean and pre-process datasets before applying additional machine learning techniques.\n",
    "\n",
    "- **Novelty detection:** Unlike outlier detection, which includes examples of both standard and anomaly events, novelty detection algorithms have only the standard event data points (i.e., no anomaly events) during training time. During training, we provide these algorithms with labeled examples of standard events (supervised learning). At testing/prediction time novelty detection algorithms must detect when an input data point is an outlier.\n",
    "\n",
    "sources: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1. https://pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/\n",
    "2. https://www.tensorflow.org/tutorials/generative/autoencoder\n",
    "\n",
    "https://www.guru99.com/autoencoder-deep-learning.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d236e7ca60ebce128ac9b571d3eda13f931e6de779d9b7dd790f3e9dce097bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
