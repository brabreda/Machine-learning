{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines and trains an autoencoder model using the TensorFlow library and the Keras API. An autoencoder is a type of neural network that is used for dimensionality reduction and feature learning.\n",
    "\n",
    "The autoencoder has an encoder and a decoder part, which are both defined in this code. The encoder part of the model consists of a series of convolutional and max pooling layers that process the input image and extract features from it. The decoder part of the model consists of a series of convolutional and upsampling layers that reconstruct the input image from the encoded representation.\n",
    "\n",
    "The autoencoder is trained to reconstruct the input image from the encoded representation, and the encoded representation is used as a compact representation of the input image. In this case, the input images are RGB images of size 128x128 pixels, and the autoencoder is trained to minimize the binary cross-entropy loss between the input and the reconstructed images using the Adam optimizer.\n",
    "\n",
    "The code also defines an ImageDataGenerator object to load and preprocess the images from a directory. The images are divided into a training set and a validation set, and the autoencoder is trained on the training set and evaluated on the validation set.\n",
    "\n",
    "The code also defines a ModelCheckpoint callback, which saves the weights of the model every 5 epochs during training. Finally, the code fits the autoencoder to the training data using the fit() method, and generates latent representations of the food images using the encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "# Set the input shape and number of classes\n",
    "INPUT_SHAPE = (128, 128, 3)  # Assume 128x128 RGB images\n",
    "NUM_CLASSES = 2  # Food or not food\n",
    "\n",
    "# Set the batch size\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 1 classes.\n",
      "Found 199 images belonging to 1 classes.\n",
      "(64, 128, 128, 3)\n",
      "(64,)\n",
      "(64, 128, 128, 3)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set the directory containing the images\n",
    "image_dir = 'food_images_2'\n",
    "\n",
    "# Create an ImageDataGenerator object\n",
    "image_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Use the generator to load the images from the directory\n",
    "train_data_gen = image_generator.flow_from_directory(image_dir,\n",
    "                                                     target_size=(128, 128),\n",
    "                                                     color_mode='rgb',\n",
    "                                                     class_mode='binary',\n",
    "                                                     batch_size=64,\n",
    "                                                     subset='training')\n",
    "val_data_gen = image_generator.flow_from_directory(image_dir,\n",
    "                                                   target_size=(128, 128),\n",
    "                                                   color_mode='rgb',\n",
    "                                                   class_mode='binary',\n",
    "                                                   batch_size=64,\n",
    "                                                   subset='validation')\n",
    "\n",
    "# Get the images and labels from the generator\n",
    "x_train, y_train = next(train_data_gen)\n",
    "x_val, y_val = next(val_data_gen)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 128, 128, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 64, 64, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 64, 64, 8)         1160      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 32, 32, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 32, 32, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 16, 16, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 16, 16, 8)         584       \n",
      "                                                                 \n",
      " up_sampling2d_15 (UpSamplin  (None, 32, 32, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 32, 32, 8)         584       \n",
      "                                                                 \n",
      " up_sampling2d_16 (UpSamplin  (None, 64, 64, 8)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 64, 64, 16)        1168      \n",
      "                                                                 \n",
      " up_sampling2d_17 (UpSamplin  (None, 128, 128, 16)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 128, 128, 3)       435       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,963\n",
      "Trainable params: 4,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "encoder_input = layers.Input(shape=INPUT_SHAPE)\n",
    "x = layers.Conv2D(16, 3, activation='relu', padding='same')(encoder_input)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(8, 3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(8, 3, activation='relu', padding='same')(x)\n",
    "encoder_output = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "# Define the decoder\n",
    "decoder_input = encoder_output\n",
    "x = layers.Conv2D(8, 3, activation='relu', padding='same')(decoder_input)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "x = layers.Conv2D(8, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "x = layers.Conv2D(16, 3, activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "decoder_output = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = tf.keras.Model(encoder_input, encoder_output)\n",
    "\n",
    "# Define the autoencoder\n",
    "autoencoder = tf.keras.Model(encoder_input, decoder_output)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the autoencoder with a loss function and an optimizer\n",
    "autoencoder.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))\n",
    "\n",
    "\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6935 - val_loss: 0.6935\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.6935 - val_loss: 0.6934\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 1s 952ms/step - loss: 0.6934 - val_loss: 0.6934\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 1s 926ms/step - loss: 0.6934 - val_loss: 0.6933\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 1s 849ms/step - loss: 0.6933 - val_loss: 0.6933\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint('autoencoder_weights.h5', save_weights_only=True, period=5)\n",
    "\n",
    "# Fit the autoencoder to the training data, using the ModelCheckpoint callback\n",
    "history = autoencoder.fit(x_train,\n",
    "                          x_train,\n",
    "                          epochs=NUM_EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          validation_data=(x_val, x_val),\n",
    "                          callbacks=[checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "X_encoded = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2/2 [==============================] - 1s 4ms/step - loss: 0.4623 - accuracy: 0.6094\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 4.2177e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164c31660>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "# Train a classifier on the lower-dimensional representation of the input images\n",
    "classifier = Sequential()\n",
    "classifier.add(Flatten(input_shape=X_encoded.shape[1:]))  # Add a Flatten layer to reshape the input\n",
    "classifier.add(Dense(32, activation='relu'))\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.fit(X_encoded, y_train, epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[[0.00018595]\n",
      " [0.00018602]\n",
      " [0.000186  ]\n",
      " [0.00018593]\n",
      " [0.00018602]\n",
      " [0.000186  ]\n",
      " [0.00018594]\n",
      " [0.00018602]\n",
      " [0.00018601]\n",
      " [0.00018594]\n",
      " [0.00018603]\n",
      " [0.00018601]\n",
      " [0.00018594]\n",
      " [0.00018602]\n",
      " [0.00018601]\n",
      " [0.00018594]\n",
      " [0.00018602]\n",
      " [0.00018601]\n",
      " [0.00018594]\n",
      " [0.00018602]\n",
      " [0.000186  ]\n",
      " [0.00018593]\n",
      " [0.00018602]\n",
      " [0.00018607]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/gsvsrpxd2kqb4f6v1wy203b80000gn/T/ipykernel_5655/1346691681.py:6: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image_data = imageio.imread('./tripadvisor_images_2/23487213_1.jpg')\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Load the image and convert it to a NumPy array\n",
    "image_data = imageio.imread('./tripadvisor_images_2/23487213_1.jpg')\n",
    "\n",
    "# Resize the image to the same size as the training images\n",
    "resized_image = resize(image_data, (128, 128, 3))\n",
    "\n",
    "# Add an extra dimension to the array to match the expected input shape of the model\n",
    "resized_image = np.expand_dims(resized_image, axis=0)\n",
    "\n",
    "resized_image /= 255.\n",
    "\n",
    "# Use the encoder to generate a lower-dimensional representation of the image\n",
    "img_encoded = autoencoder.predict(resized_image)\n",
    "\n",
    "# Use the classifier to predict the class of the image\n",
    "prediction = classifier.predict(img_encoded)\n",
    "print(prediction)  # Will print either [0] or [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "prediction = np.argmax(prediction)  # Will print either 0 or 1\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
