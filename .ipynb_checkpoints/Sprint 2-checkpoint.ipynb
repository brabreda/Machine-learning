{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a363ecd",
   "metadata": {},
   "source": [
    "# Machine learning - sprint 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23d2c5",
   "metadata": {},
   "source": [
    "Authors: Allart Ewoud, Van Hees Maxime, Breda Bram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e329619",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421439c",
   "metadata": {},
   "source": [
    "The main focus of this paper is going to be about making prediction based on the images of the restaurant listings. We see alot of oppertunities where this can come in handy such as: \n",
    "- selecting the best pictures from a listing (the one where the model predicts the highest rating)\n",
    "- creating a model that can predict the cuisine types of a restaurant based om the images that are available for that restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9aac2",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0174188",
   "metadata": {},
   "source": [
    "To start off we're importing all the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "import glob\n",
    "import PIL\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv(\"tripadvisor_dataset/restaurant_listings.csv\")\n",
    "\n",
    "# display the data and see how it formulated\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df = original_df.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fb45f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db7e35",
   "metadata": {},
   "source": [
    "Our main focus of this assignment is about making predictions based on pictures, so we start off by creating a dataframe that includes our pictures. This step includes joining the pictures togheter with the other data from sprint 1. We continue by extracting features from the pictures, we do this with the HOG method. \n",
    "\n",
    "Als laatste reduceren we de dimenties van de features met PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general rating\n",
    "df = df.drop(columns=[\"atmosphere rating\"])\n",
    "\n",
    "df[\"general rating\"] = original_df[\"general rating\"].apply(lambda x: float(str(x).split(' ')[0]))\n",
    "df[\"general rating\"] = pd.to_numeric(df[\"general rating\"])\n",
    "mean = df[\"general rating\"].loc[df[\"general rating\"] != -1].mean()\n",
    "df[\"general rating\"] = df[\"general rating\"].replace(-1,mean)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ffc48",
   "metadata": {},
   "source": [
    "### Joining pixels to data from sprint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd558d4",
   "metadata": {},
   "source": [
    "This step consists of creating a dataframe with the pixels of every image. During this step we noticed that some pictures where invalid, they coudn't be opened, this is also handled. At last we join the dataframe with the images togheter with the dataframe of sprint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of all images\n",
    "fileNameList = glob.glob(\"tripadvisor_dataset/tripadvisor_images/*.jpg\")\n",
    "images = []\n",
    "\n",
    "# putting all images in a dict where the key is the name of the picture\n",
    "for fileName in tqdm(fileNameList, total=len(fileNameList)):\n",
    "    try:\n",
    "        img = Image.open(f\"{fileName}\")\n",
    "        img = img.resize((128,128))\n",
    "        img_np = np.array(img).flatten()\n",
    "        #TODO resize all the images\n",
    "        #images.append(pd.Series(data=[fileName.split('/')[-1].split(\"_\")[0], img_np]))\n",
    "        images.append(pd.Series(data=[fileName.split('\\\\')[-1].split(\"_\")[0], img_np]))\n",
    "    except PIL.UnidentifiedImageError:\n",
    "        pass\n",
    "\n",
    "# changing the list of pd.Series to pandas dataframe\n",
    "images = pd.concat(images, axis=1).T\n",
    "images = images.rename(columns={0 : \"id\", 1 : \"pixels\"})\n",
    "images[\"id\"] = pd.to_numeric(images[\"id\"])\n",
    "\n",
    "# merging 2 dataframes\n",
    "df = pd.merge(df, images, on=\"id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853633ef",
   "metadata": {},
   "source": [
    "### HOG feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import swifter\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def hog_transformer(img):\n",
    "    if len(img) != 49152:\n",
    "        return np.empty(0)\n",
    "    \n",
    "    img = img.reshape(128,128,3)\n",
    "    \n",
    "    fd, hog_image = hog(img,orientations=8, pixels_per_cell=(16, 16),\n",
    "                   cells_per_block=(1, 1), visualize=True, channel_axis=-1)\n",
    "    return np.array(fd).flatten()\n",
    "\n",
    "# Use swifter to parrallelize the apply, if possible\n",
    "df['hog'] = df['pixels'].swifter.apply(hog_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beae714",
   "metadata": {},
   "source": [
    "### Cleaning up the images that are to small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fca0f",
   "metadata": {},
   "source": [
    "We noticed that some pictures are to small to resize to the 128 by 128 format so we excluded those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pixel_size\"] = df[\"pixels\"].apply(lambda x: len(x))\n",
    "df = df.drop(df[df.pixel_size != 49152].index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b3573",
   "metadata": {},
   "source": [
    "The last step of preprocessing is splitting the data in a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f680c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, random_state=0, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326a0fb",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7d814",
   "metadata": {},
   "source": [
    "For the first case we are going to look **if we can predict scores based on the images of the listings**. One of the first things where this can come to use is for choosing the best picture of an listing. Another thing that comes to mind is by determining if the pictures corresponds to the score. This can for example be used to tell the restaurants if they have bad or good pictures.\n",
    "\n",
    "De predictie zal op basis van regressie zijn, en de predicte score kan een soort van metriek vormen die vertelt hoe goed de fotos van een restorant scoren tov van de andere restaurants.\n",
    "\n",
    "**gevraagd aan de leerkracht en was een intressant idee, een goede combinatie is om dit eventueel te bekijken in combinatie met prijsklasse of price range**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# kga deze stap nog proberen toe te voegen aan het dataframe\n",
    "# hogs uit het dataframe halen voor de scaler\n",
    "train_images_hogs = np.stack(df_train['hog'].values)\n",
    "train_hogs_scaled = StandardScaler().fit_transform(train_images_hogs)\n",
    "\n",
    "train_images_hogs = np.stack(df_test['hog'].values)\n",
    "train_hogs_scaled = StandardScaler().fit_transform(train_images_hogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860b454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# juiste PCA componentes zoeken van de grafiek\n",
    "pca = PCA()\n",
    "data_reduced = pca.fit_transform(train_hogs_scaled)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b861f5",
   "metadata": {},
   "source": [
    "TODO: model opbouwen en eventueel verbeteren. je kan normaal hogs gebruiken als x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7f2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we namen als n_components 350\n",
    "pca = PCA(n_components = 350)\n",
    "# de getransformeerde data zijn onze features\n",
    "x_train = pca.fit_transform(train_images_hogs)\n",
    "y_train = df_train['general rating'].values\n",
    "\n",
    "x_test = pca.fit_transform(test_images_hogs)\n",
    "y_test = df_train['general rating'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a122e28",
   "metadata": {},
   "source": [
    "## Case 2\n",
    "\n",
    "Creating a model that can predict the cuisine types of a restaurant based om the images that are available for that restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102b155",
   "metadata": {},
   "source": [
    "In Sprint 1 we discovered that the tags column sometimes holds cuisine types that are not listed in the cuisines column, but because this were only a few cuisines, and none of them appeared more than 5 times, we don't need to add these to our dataframe. This is the case beacause with only so few data to learn from, it is not possible to train a good ML model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f04e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to create a dataframe that contains the id of a restaurant and one of the cuisines on each row\n",
    "df_with_cuisines = original_df[['id', 'cuisines']]\n",
    "df_with_cuisines['cuisines'] = df_with_cuisines['cuisines'].apply(lambda x: str(x).split(','))\n",
    "df_with_cuisines = df_with_cuisines.explode('cuisines', ignore_index=True)\n",
    "df_with_cuisines = pd.merge(df_with_cuisines, df, on=\"id\")\n",
    "df_with_cuisines['cuisines'].apply(lambda x: x.strip())\n",
    "df_with_cuisines['cuisine'] = df_with_cuisines['cuisines'].apply(lambda x: x.strip())\n",
    "df_with_cuisines.drop(columns=['cuisines'], inplace=True)\n",
    "cuisine_counts = dict(df_with_cuisines['cuisine'].value_counts())\n",
    "\n",
    "# we only keep the cuisines that occur more than 100 times, to be able to train a descent model on them\n",
    "# a google search learned us that 100 is a good number to start with\n",
    "cuisines_subset = { key: value for (key,value) in cuisine_counts.items() if value > 100 }\n",
    "\n",
    "# now we replace all values that are not in the cuisines_subset with 'other'\n",
    "df_with_cuisines['cuisine'] = df_with_cuisines['cuisine'].apply(lambda x: x if x in cuisines_subset else 'Other')\n",
    "\n",
    "\n",
    "print(\"Value counts: \\n\") \n",
    "print(df_with_cuisines['cuisine'].value_counts())\n",
    "\n",
    "print('\\nDifferent amount of cuisines: ', len(df_with_cuisines['cuisine'].value_counts()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d5fff",
   "metadata": {},
   "source": [
    "We will now split the data in train and test data, with input and output data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2447642",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_train, x2_test, y2_train, y2_test = train_test_split(df_with_cuisines['hog'], df_with_cuisines['cuisine'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a4038",
   "metadata": {},
   "source": [
    "Now, we can train and test different machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
